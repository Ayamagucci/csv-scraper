require('dotenv').config();
const { PORT } = process.env;
const express = require('express');
const fs = require('fs');
const axios = require('axios');
const puppeteer = require('puppeteer');
const createCsvWriter = require('csv-writer').createObjectCsvWriter;

const app = express();

// scrape jobs via Puppeteer (instead of cheerio)
const scrapeJobs = async(browser, page) => {
  try {
    // generate URL based on page num
    const site = page === 1
      ? (`https://www.ranchwork.com/jcategory/all-ranch-jobs/`)
      : (`https://www.ranchwork.com/jcategory/all-ranch-jobs/page/${ page }/`);

    // navigate to pages
    const jobPage = await browser.newPage();
    await jobPage.goto(site, { waitUntil: 'domcontentloaded' });

    const jobElems = await jobPage.$$('.job-item');

    // store scraped data into array
    const jobs = [];

    // iterate through all jobs
    for (const jobElem of jobElems) {
      // skip filled / expired jobs
      if (await jobElem.$('.filled-expired-stamp')) {
        continue;
      }

      // desired fields
      const title = await jobElem.$eval('a', (elem) => elem.getAttribute('title'));
      const company = await jobElem.$eval('.job-company', (elem) => elem.textContent.trim());
      const place = await jobElem.$eval('.job-place', (elem) => elem.textContent.trim());
      const type = await jobElem.$eval('.job-type', (elem) => elem.textContent.trim());
      const date = await jobElem.$eval('.job-date', (elem) => elem.textContent.trim());

      // extract data
      jobs.push({ title, company, place, type, date });
    }

    // close browser
    await browser.close();

    // output scraped jobs
    console.log(`Jobs on page ${ page } successfully scraped!`);
    console.dir(jobs);

    return jobs;

  } catch(err) {
    console.error(`Error scraping jobs on page ${ page }: ${ err.message }`);
    return [];
  }
};

const writeJobsToCsv = async(data) => {
  // create CSV writer w/ specified header fields
  const csvWriter = createCsvWriter({
    path: 'csv/jobs.csv',
    header: [
      { id: 'title', title: 'Title' },
      { id: 'company', title: 'Company' },
      { id: 'place', title: 'Place' },
      { id: 'type', title: 'Type' },
      { id: 'date', title: 'Date' }
    ]
  });

  try {
    await csvWriter.writeRecords(data);

    // output message indicating success
    console.log('Jobs successfully written to CSV file!');
    // console.dir(data);

  } catch(err) {
    console.error(`Error writing data to CSV: ${ err }`);
  }
};

const scrapeMultiplePages = async(numPages) => {
  const allJobs = [];

  try {
    const browser = await puppeteer.launch({ headless: 'new' });

    // iterate through all pages
    for (let page = 1; page <= numPages; page++) {
      const jobs = await scrapeJobs(browser, page);

      if (jobs) {
        allJobs.push(...jobs);
      }
    }

    // close browser instance & return jobs
    await browser.close();
    return allJobs;

  } catch(err) {
    console.error(`Error during scraping: ${ err.message }`);
    return [];
  }
};

// scrape site â€”> write data to CSV
(async() => {
  // write jobs to CSV **
  const jobs = await scrapeMultiplePages(49);
  writeJobsToCsv(jobs);
})();

app.listen(PORT, () => {
  console.log(`Server listening on PORT: ${ PORT }`);
});